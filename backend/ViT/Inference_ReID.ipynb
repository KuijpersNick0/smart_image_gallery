{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shop\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\shop\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "c:\\Users\\shop\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from easydict import EasyDict as edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    '''\n",
    "    A class for inference only\n",
    "    Args:\n",
    "        model(lightreid.ReIDModel)\n",
    "        img_size(tuple): height, width\n",
    "        model_path(str): path to model_state_dict.pth\n",
    "        use_gpu(bool): use gpu to extract features\n",
    "        light_feat(bool): if True, the model output is binary code [0,1]\n",
    "    ExamplesL:\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, img_size, model_path, use_gpu=False, light_feat=False, **kwargs):\n",
    "        '''\n",
    "        Args:\n",
    "            model(lightreid.BaseReIDModel)\n",
    "            img_size(tuple): height, width\n",
    "            use_gpu(bool): True or False\n",
    "        '''\n",
    "        self.height, self.width = img_size\n",
    "        self.device = torch.device('cuda') if use_gpu else torch.device('cpu')\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406] if 'mean' not in kwargs.keys() else kwargs['mean']\n",
    "        std = [0.229, 0.224, 0.225] if 'std' not in kwargs.keys() else kwargs['std']\n",
    "        self.set_mean(mean=mean)\n",
    "        self.set_std(std=std)\n",
    "\n",
    "        # init\n",
    "        model = self.resume_from_path(model, model_path)\n",
    "        if light_feat:\n",
    "            print('enable tanh, output binary code')\n",
    "            model.enable_tanh()\n",
    "        self.model = model.to(self.device).eval()\n",
    "\n",
    "\n",
    "    def set_mean(self, mean):\n",
    "        print('set mean {}'.format(mean))\n",
    "        self.mean = mean\n",
    "\n",
    "    def set_std(self, std):\n",
    "        print('set std {}'.format(std))\n",
    "        self.std = std\n",
    "\n",
    "\n",
    "    def process(self, inputs, return_type='numpy'):\n",
    "        '''\n",
    "        Args:\n",
    "            inputs: accept\n",
    "                torch.tensor: image(s) of size [(num), 3, h, w] normalized by ImageNet mean and std, RGB format\n",
    "                np.ndarray: image(s) of size [(num), 3, h, w] in range [0, 255], RGB format\n",
    "                str: an image path\n",
    "                list(str): a list of image paths\n",
    "            return_type(str): accept\n",
    "                'numpy': np.array of size [num, dim]\n",
    "                'torch': torch.tensor of size [num, dim]\n",
    "        '''\n",
    "        imgs = self.process_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            feats = self.model(imgs)\n",
    "\n",
    "        if return_type == 'numpy':\n",
    "            return feats.data.cpu().numpy()\n",
    "        elif return_type == 'torch':\n",
    "            return feats.data\n",
    "\n",
    "\n",
    "    def process_inputs(self, inputs):\n",
    "        '''\n",
    "        process inputs to formal format as below:\n",
    "        Args:\n",
    "            inputs: accept\n",
    "                torch.tensor: image(s) of size [(num), 3, h, w] normalized by ImageNet mean and std\n",
    "                np.ndarray: image(s) of size [(num), 3, h, w] in range [0, 255]\n",
    "                str: an image path\n",
    "                list(str): a list of image paths\n",
    "        Return:\n",
    "            outputs(torch.tensor): [num, 3, h, w], normalized with ImageNet mean and std in RGB\n",
    "        '''\n",
    "\n",
    "        # if inputs are np.ndarray of size [3,h,w] or [num,3,h,w]\n",
    "        if isinstance(inputs, np.ndarray):\n",
    "            if len(inputs.shape) == 3:\n",
    "                outputs = np.expand_dims(inputs, dim=0)\n",
    "            elif len(inputs.shape) == 4:\n",
    "                outputs = inputs\n",
    "            else:\n",
    "                raise RuntimeError('inputs dimension error, expect 3([3, h, w]) or 4(num, 3, h, w), but got {}'.format(len(inputs.shape)))\n",
    "\n",
    "        # if inputs are torch.tensor\n",
    "        elif isinstance(inputs, torch.Tensor):\n",
    "            if len(inputs.shape) == 3:\n",
    "                outputs = inputs.unsqueeze(0)\n",
    "            elif len(inputs.shape) == 4:\n",
    "                outputs = inputs\n",
    "            else:\n",
    "                raise RuntimeError('inputs dimension error, expect 3([3, h, w]) or 4(num, 3, h, w), but got {}'.format(len(inputs.shape)))\n",
    "            outputs = outputs.to(self.device)\n",
    "            return outputs\n",
    "\n",
    "        # if inputs is path(s)\n",
    "        elif isinstance(inputs, str): # if input a path\n",
    "            img = Image.open(inputs)  # load img as [0, 255], HWC, RGB\n",
    "            img = img.resize([self.width, self.height], resample=2)  # resize\n",
    "            img_numpy = np.array(img)  # astype numpy\n",
    "            outputs = np.expand_dims(img_numpy, axis=0)\n",
    "            outputs = outputs.transpose([0, 3, 1, 2])  # [num, h, w, 3] --> [num, 3, h, w]\n",
    "        elif isinstance(inputs, list): # if input a list of path\n",
    "            outputs = []\n",
    "            for input in inputs:\n",
    "                img = Image.open(input) # load img as [0, 255], HWC, RGB\n",
    "                img = img.resize([self.width, self.height], resample=2) # resize\n",
    "                img_numpy = np.array(img) # astype numpy\n",
    "                outputs.append(img_numpy)\n",
    "            outputs = np.asarray(outputs)\n",
    "            outputs = outputs.transpose([0, 3, 1, 2])  # [num, h, w, 3] --> [num, 3, h, w]\n",
    "\n",
    "        outputs = outputs / 255. # [0, 255] --> [0, 1]\n",
    "        # normalize with Imagenet mean and std\n",
    "        mean = np.array(self.mean).reshape([1, -1, 1, 1])\n",
    "        std = np.array(self.std).reshape([1, -1, 1, 1])\n",
    "        outputs = (outputs - mean) / std\n",
    "\n",
    "        outputs = torch.from_numpy(outputs).float().to(self.device)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def resume_from_path(self, model, model_path):\n",
    "        '''resume from model. model_path shoule be like /path/to/model.pkl'''\n",
    "        # self.model.load_state_dict(torch.load(model_path), strict=False)\n",
    "        # print(('successfully resume model from {}'.format(model_path)))\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "        model_dict = model.state_dict()\n",
    "        new_state_dict = OrderedDict()\n",
    "        matched_layers, discarded_layers = [], []\n",
    "        for k, v in state_dict.items():\n",
    "            if k.startswith('module.'):\n",
    "                k = k[7:]  # discard module.\n",
    "            if k in model_dict and model_dict[k].size() == v.size():\n",
    "                new_state_dict[k] = v\n",
    "                matched_layers.append(k)\n",
    "            else:\n",
    "                discarded_layers.append(k)\n",
    "        model_dict.update(new_state_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        if len(discarded_layers) > 0:\n",
    "            print('discarded layers: {}'.format(discarded_layers))\n",
    "        return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
